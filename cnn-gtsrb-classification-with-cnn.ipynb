{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [CNN]-[GTSRB] - Classification of GTSRB Dataset Using Convolutional Neural Networks (CNN)\n\nThis notebook delves into a classification task involving the use of Convolutional Neural Networks (CNNs) on the GTSRB dataset.\n\nThe German Traffic Sign Recognition Benchmark (GTSRB) is a dataset comprising over 50,000 photos of road signs categorized into approximately 40 classes.\n\nA detailed description of the dataset can be found at: [http://benchmark.ini.rub.de/](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)\n\nWe have structured the notebook into two main sections:\n\n# Objectives\nThis section delineates the specific goals of this notebook, which are:\n\n- Training a Deep Neural Network (DNN) model to achieve high accuracy in classification of road signs.\n\n# Implementation\nThis section presents the hands-on steps necessary to attain the previously mentioned objectives. These steps include:\n\n- **Imports, Constants, and Methods:** Setting up the necessary libraries, constants, and methods for our task.\n- **Data Retrieval:** Acquiring the GTSRB dataset to be used for training and testing purposes.\n- **Data Preparation:** Preprocessing and setting up the dataset to facilitate effective training of the CNN model.\n- **Model Creation:** Architecting and constructing the CNN model utilizing Keras.\n- **Model Training:** Engaging the CNN model in learning using the prepared dataset.\n- **Evaluation:** Gauging the trained model's performance and analyzing the classification results.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports & Constants & Methods\n### 1. Import","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\n\nimport os  # For operating system related functionalities\nimport time  # For time-related functionalities\nimport sys  # For system-specific parameters and functions\nimport csv  # For reading and writing CSV files\nimport math  # For mathematical operations\nimport random  # For generating random numbers\nimport numpy as np  # For numerical operations on arrays\nimport pandas as pd  # For data manipulation and analysis\nimport matplotlib.pyplot as plt  # For creating plots and visualizations\nimport h5py  # For working with HDF5 files\nimport ipywidgets as widgets  # For creating interactive widgets in Jupyter Notebook\nfrom ipywidgets import IntProgress\nfrom IPython.display import display, Markdown  # For displaying outputs and Markdown text\n\nimport visualkeras # Model visual\n\nimport skimage  # For image processing operations\nfrom skimage import io, color, transform  # For reading and manipulating images\n\nimport tensorflow as tf  # For building and training machine learning models\nfrom tensorflow import keras  # High-level API for TensorFlow\nimport json  # For working with JSON data\nfrom IPython.display import display  # For displaying outputs in Jupyter Notebook\n\nimport warnings  # For ignoring warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:03:27.359160Z","iopub.execute_input":"2023-07-17T19:03:27.359607Z","iopub.status.idle":"2023-07-17T19:03:36.850537Z","shell.execute_reply.started":"2023-07-17T19:03:27.359573Z","shell.execute_reply":"2023-07-17T19:03:36.848668Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Requirement already satisfied: visualkeras in /opt/conda/lib/python3.10/site-packages (0.0.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (9.5.0)\nRequirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (1.23.5)\nRequirement already satisfied: aggdraw>=1.3.11 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (1.3.16)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.2. Constants\nGenerating datasets can be a time and space-consuming process, taking approximately **10 minutes** and requiring **10 GB** of storage space.\n\nTo customize the dataset generation, you have the option to perform tests or generate the entire enhanced dataset by adjusting the following parameters:\n\n- `datasets`: This is a list of datasets to be used for the model training. In the complete run, we are using 'set-48x48-RGB' dataset.\n- `models`: This is a dictionary of model getter function names keyed by their identifiers. In the complete run, we are using the 'get_model_v2' model.\n- `batch_size`: The number of training examples utilized in one iteration. In the complete run, we're using a batch size of 64.\n- `epochs`: An epoch is a measure of the number of times all of the training vectors are used once to update the weights. For the complete run, we have 20 epochs.\n- `scale`: This value specifies the proportion of the dataset to use for the model training. A scale of 1 means that 100% of the dataset is used. In the complete run, we are using the entire dataset (scale = 1).\n- `with_datagen`: This boolean value specifies whether to use a data generator for data augmentation. In the complete run, we have set this to True, meaning a data generator will be used.\n\n\nVerbosity during training:\n- 0: Silent mode, no output will be displayed during training.\n- 1: Progress bar mode, a progress bar will be displayed to show the progress of each epoch.\n- 2: One line per epoch mode, a concise summary will be displayed for each epoch.","metadata":{}},{"cell_type":"code","source":"# Fast tests\n# ----\ndatasets_size = [24]\ndatasets = ['set-24x24-RGB']\nmodels        = {'v1':'get_model_v1', 'v2':'get_model_v2', 'v3':'get_model_v3'}\nbatch_size    = 64\nepochs        = 5\nscale         = 0.1\nwith_datagen  = False\n\n# All possibilities\n# ---- \n# datasets_size = [24, 48]\n# datasets      = [set-24x24-RGB', 'set-48x48-RGB']\n# models        = {'v1':'get_model_v1', 'v2':'get_model_v2', 'v3':'get_model_v3'}\n# batch_size    = 64\n# epochs        = 16\n# scale         = 1\n# with_datagen  = False\n\n# Complete\n# ---- \n#datasets_size = [24, 48]\n#datasets      = ['set-48x48-RGB']\n#models        = {'v2':'get_model_v2'}\n#batch_size    = 64\n#epochs        = 20\n#scale         = 1\n#with_datagen  = True\n\n# Global\n# ---- \ntag_id = '{}'.format(random.randint(0,99999))\n\n# Verbosity\n# ----\nverbosity = 2\n\n# Patch\n\n# ----\nrun_dir = './run' \ndata_output_dir = './data' \nproject_gtsrb_path = \"/kaggle/input/gtsrb-german-traffic-sign\"\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:03:36.853616Z","iopub.execute_input":"2023-07-17T19:03:36.853976Z","iopub.status.idle":"2023-07-17T19:03:36.862502Z","shell.execute_reply.started":"2023-07-17T19:03:36.853945Z","shell.execute_reply":"2023-07-17T19:03:36.860751Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Methods","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------ #\n# Global Methods\n# ------------------------------------------------ #\n    \n# ---------------- #    \n# Images\n# ---------------- #    \n\ndef preprocess_images(images, width=25, height=25):\n    '''\n    Resize and convert images - doesn't change originals.\n    input images must be RGBA or RGB.\n    Note: all outputs are fixed-size numpy arrays of float64.\n    \n    Args:\n        images: list of images\n        width, height: new image size (default: 25x25)\n        \n    Returns:\n        numpy array of enhanced images\n    '''\n    out = []\n    \n    loading_max = len(images)\n    progress_bar = widgets.IntProgress(min=0, max=loading_max, bar_style='info')\n    display(progress_bar) \n        \n    for i, img in enumerate(images):\n        \n        # If RGBA, convert to RGB\n        if img.shape[2] == 4:\n            img = color.rgba2rgb(img)\n            \n        # Resize\n        img = transform.resize(img, (width, height))\n\n        # Add image to the list\n        out.append(img)\n        progress_bar.value = i + 1\n    progress_bar.close()\n\n    # Reshape images\n    out = np.array(out, dtype='float64')\n    out = out.reshape(-1, width, height, 3)\n    \n    return out\n\n\n# ---------------- #    \n# Show \n# ---------------- # \n\ndef show_text(heading_level, text):\n    '''\n    Display a markdown heading or bold text\n    args:\n        heading_level : heading level (h1, h2, h3, h4, h5, b)\n        text : text to display\n    return:\n        none\n    '''\n    # Switch the heading level\n    # ----\n    if heading_level == 'h1':\n        display(Markdown(f'# {text}'))\n    elif heading_level == 'h2':\n        display(Markdown(f'## {text}'))\n    elif heading_level == 'h3':\n        display(Markdown(f'### {text}'))\n    elif heading_level == 'h4':\n        display(Markdown(f'#### {text}'))\n    elif heading_level == 'h5':\n        display(Markdown(f'##### {text}'))\n    elif heading_level == 'b':\n        display(Markdown(f'**{text}**'))\n    else:\n        display(Markdown(f'{text}'))\n        \ndef show_data_and_sample_images(samples):\n    show_text(\"b\",'Data')\n    show_images(\n        x_train,\n        y_train, \n        samples, \n        columns=10, \n        figure_size=(2,2), \n        show_colorbar=False, \n        y_pred=None, \n        color_map='binary'\n    )\n    show_text(\"b\",'Meta')\n    show_images(\n        x_meta,\n        y_meta, \n        y_train[samples], \n        columns=10, \n        figure_size=(2,2), \n        show_colorbar=False, \n        y_pred=None, \n        color_map='binary'\n    )\n    \n    \ndef show_images(\n    x, \n    y=None, \n    indices='all', \n    columns=12, \n    figure_size=(1, 1),                \n    show_colorbar=False, \n    y_pred=None, \n    color_map='binary',\n    normalization=None, \n    padding=0.35, \n    spines_alpha=1, \n    font_size=20,\n    interpolation='lanczos'\n):\n    \"\"\"\n    Display a grid of images with labels.\n\n    Args:\n        x: The images to display. Shapes must be (-1, lx, ly), (-1, lx, ly, 1), or (-1, lx, ly, 3).\n        y: Real classes or labels associated with the images. (None)\n        indices: Indices of images to show or 'all' for all images. ('all')\n        columns: Number of columns in the grid. (12)\n        figure_size: Size of the figure (width, height). (1, 1)\n        show_colorbar: Whether to show the colorbar. (False)\n        y_pred: Predicted classes associated with the images. (None)\n        color_map: Matplotlib color map to use. ('binary')\n        normalization: Matplotlib imshow normalization. (None)\n        padding: Padding between rows in the grid. (0.35)\n        spines_alpha: Alpha value for the spines. (1)\n        font_size: Font size in pixels. (20)\n        interpolation: Interpolation method for displaying the images. ('lanczos')\n\n    Returns:\n        None\n    \"\"\"\n    if indices == 'all':\n        indices = range(len(x))\n\n    if normalization and len(normalization) == 2:\n        normalization = matplotlib.colors.Normalize(vmin=normalization[0], vmax=normalization[1])\n\n    draw_labels = (y is not None)\n    draw_predicted_labels = (y_pred is not None)\n\n    rows = math.ceil(len(indices) / columns)\n    fig = plt.figure(figsize=(columns * figure_size[0], rows * (figure_size[1] + padding)))\n\n    n = 1\n    for i in indices:\n        axs = fig.add_subplot(rows, columns, n)\n        n += 1\n\n        # Shape is (lx,ly)\n        # ----\n        if len(x[i].shape) == 2:\n            xx = x[i]\n        # Shape is (lx,ly,n)\n        # ----\n        if len(x[i].shape) == 3:\n            (lx, ly, lz) = x[i].shape\n            if lz == 1:\n                xx = x[i].reshape(lx, ly)\n            else:\n                xx = x[i]\n\n        img = axs.imshow(xx, cmap=color_map, norm=normalization, interpolation=interpolation)\n\n        axs.spines['right'].set_visible(True)\n        axs.spines['left'].set_visible(True)\n        axs.spines['top'].set_visible(True)\n        axs.spines['bottom'].set_visible(True)\n\n        axs.spines['right'].set_alpha(spines_alpha)\n        axs.spines['left'].set_alpha(spines_alpha)\n        axs.spines['top'].set_alpha(spines_alpha)\n        axs.spines['bottom'].set_alpha(spines_alpha)\n\n        axs.set_yticks([])\n        axs.set_xticks([])\n\n        if draw_labels and not draw_predicted_labels:\n            axs.set_xlabel(y[i], fontsize=font_size)\n        if draw_labels and draw_predicted_labels:\n            if y[i] != y_pred[i]:\n                axs.set_xlabel(f'{y_pred[i]} ({y[i]})', fontsize=font_size)\n                axs.xaxis.label.set_color('red')\n            else:\n                axs.set_xlabel(y[i], fontsize=font_size)\n\n        if show_colorbar:\n            fig.colorbar(img, orientation=\"vertical\", shrink=0.65)\n\n    plt.show()\n\ndef highlight_max(s):\n    is_max = (s == s.max())\n    return ['background-color: yellow' if v else '' for v in is_max]\n\ndef show_report(file):\n    # ---- Read json file\n    with open(file) as infile:\n        dict_report = json.load( infile )\n    output      = dict_report['output']\n    description = dict_report['description']\n    # ---- about\n    show_text(\"h1\",f'Report : {Path(file).stem}')\n    print(    \"Desc.  : \",description,'\\n')\n    # ---- Create a pandas\n    report       = pd.DataFrame (output)\n    col_accuracy = [ c for c in output.keys() if c.endswith('Accuracy')]\n    col_duration = [ c for c in output.keys() if c.endswith('Duration')]\n    # ---- Build formats\n    lambda_acc = lambda x : '{:.2f} %'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n    lambda_dur = lambda x : '{:.1f} s'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n    formats = {'Size':'{:.2f} Mo'}\n    for c in col_accuracy:   \n        formats[c]=lambda_acc\n    for c in col_duration:\n        formats[c]=lambda_dur\n    t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n    display(t)\n    \n# ---------------- #     \n# Dataset \n# ---------------- # \n\ndef save_h5_dataset(x_train, y_train, x_test, y_test, x_meta,y_meta, filename):\n        \n    # Create h5 file\n    # ----\n    with h5py.File(filename, \"w\") as f:\n        f.create_dataset(\"x_train\", data=x_train)\n        f.create_dataset(\"y_train\", data=y_train)\n        f.create_dataset(\"x_test\",  data=x_test)\n        f.create_dataset(\"y_test\",  data=y_test)\n        f.create_dataset(\"x_meta\",  data=x_meta)\n        f.create_dataset(\"y_meta\",  data=y_meta)\n        \n    # Print\n    # ----\n    size=os.path.getsize(filename)/(1024*1024)\n    print('Dataset : {:24s}  shape : {:22s} size : {:6.1f} Mo   (saved)'.format(filename, str(x_train.shape),size))\n\ndef read_dataset(enhanced_dir, dataset_name):\n    '''Reads h5 dataset from dataset_dir\n    Args:\n        dataset_dir : datasets dir\n        name        : dataset name, without .h5\n    Returns:    x_train,y_train,x_test,y_test data'''\n    # ---- Read dataset\n    filename = f'{enhanced_dir}/{dataset_name}.h5'\n    size     = os.path.getsize(filename)/(1024*1024)\n\n    with  h5py.File(filename,'r') as f:\n        x_train = f['x_train'][:]\n        y_train = f['y_train'][:]\n        x_test  = f['x_test'][:]\n        y_test  = f['y_test'][:]\n\n    # ---- Shuffle\n    x_train,y_train=shuffle_np_dataset(x_train,y_train)\n\n    # ---- done\n    return x_train,y_train,x_test,y_test,size\n\ndef shuffle_np_dataset(*data):\n    \"\"\"\n    Shuffle a list of dataset\n    args:\n        *data : datasets\n    return:\n        *datasets mixed\n    \"\"\"\n    # Ramdomize\n    # ----\n    p = np.random.permutation(len(data[0]))\n    out = [ d[p] for d in data ]\n    return out[0] if len(out)==1 else out\n\n\n\ndef rescale_dataset(*data, scale=1):\n    '''\n    Rescale numpy array with 'scale' factor\n    args:\n        *data : arrays\n        scale : scale factor\n    return:\n        arrays of rescaled data\n    '''\n    print('Datasets have been resized with a factor ', scale)\n    out = [ d[:int(scale*len(d))] for d in data ]\n    return out[0] if len(out)==1 else out\n\n# ---------------- #     \n# Dir \n# ---------------- # \n\ndef mkdir(path):\n    '''\n    Create a subdirectory\n    Mode is 0750, do nothing if exist\n    args:\n        path : directory to create\n    return:\n        none\n    '''\n    os.makedirs(path, mode=0o750, exist_ok=True)\n    \n# ---------------- # \n# Road signs\n# ---------------- # \ndef read_csv_and_extract_road_sign_images_with_labels(csv_file): \n    '''\n    Reads traffic sign data from German Traffic Sign Recognition Benchmark dataset.\n    Arguments:  \n        csv filename :  Description file, Example /data/GTSRB/Train.csv\n    Returns:\n        x,y          :  np array of images, np array of corresponding labels\n    '''\n\n    path = os.path.dirname(csv_file)\n    name = os.path.basename(csv_file)\n\n    # Read csv file\n    df = pd.read_csv(csv_file, header=0)\n    \n    # Get filenames and ClassIds\n    filenames = df['Path'].to_list()\n    y = df['ClassId'].to_list()\n    x = []\n    \n    # Read images\n    loading_max = len(filenames)\n    progress_bar = IntProgress(min=0, max=loading_max, bar_style='info')\n    display(progress_bar)\n    \n    for i, filename in enumerate(filenames):\n        image = io.imread(os.path.join(path, filename))\n        x.append(image)\n        progress_bar.value = i + 1\n    \n    progress_bar.close()\n    \n    return np.array(x, dtype=object), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:03:36.864130Z","iopub.execute_input":"2023-07-17T19:03:36.864481Z","iopub.status.idle":"2023-07-17T19:03:36.912556Z","shell.execute_reply.started":"2023-07-17T19:03:36.864448Z","shell.execute_reply":"2023-07-17T19:03:36.911552Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Read the dataset\n\nIn this step, we will be working with a traffic sign recognition benchmark dataset, which you can find more information about [here](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n\nEach directory in this dataset contains a CSV file with annotations titled `GT-<ClassID>.csv` along with training images. The CSV file includes the following fields:\n\n- Filename\n- Width\n- Height\n- Roi.X1\n- Roi.Y1\n- Roi.X2\n- Roi.Y2\n- ClassId\n\nThese fields correspond to the file name of the image, the dimensions of the image, the coordinates of the region of interest, and the class ID of the traffic sign, respectively.\n\n### 3.1. Understanding the Dataset Structure\n\nThe dataset is structured in the following way: The root directory **\\<dataset_dir\\>/GTSRB/origine** contains three main subsets: **Train**, **Test**, and **Meta**.\n\nEach of these subsets includes a CSV file that contains the image annotations, and a corresponding directory filled with image files.\n\nBy understanding the layout and structure of the dataset, we can effectively load, preprocess, and use this data for model training and testing.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f'{project_gtsrb_path}/Test.csv', header=0)\ndisplay(df.head(10))","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:03:36.915021Z","iopub.execute_input":"2023-07-17T19:03:36.915356Z","iopub.status.idle":"2023-07-17T19:03:36.984503Z","shell.execute_reply.started":"2023-07-17T19:03:36.915330Z","shell.execute_reply":"2023-07-17T19:03:36.982740Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId            Path\n0     53      54       6       5      48      49       16  Test/00000.png\n1     42      45       5       5      36      40        1  Test/00001.png\n2     48      52       6       6      43      47       38  Test/00002.png\n3     27      29       5       5      22      24       33  Test/00003.png\n4     60      57       5       5      55      52       11  Test/00004.png\n5     52      56       5       5      47      51       38  Test/00005.png\n6    147     130      12      12     135     119       18  Test/00006.png\n7     32      33       5       5      26      28       12  Test/00007.png\n8     45      50       6       5      40      45       25  Test/00008.png\n9     81      86       7       7      74      79       35  Test/00009.png","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Width</th>\n      <th>Height</th>\n      <th>Roi.X1</th>\n      <th>Roi.Y1</th>\n      <th>Roi.X2</th>\n      <th>Roi.Y2</th>\n      <th>ClassId</th>\n      <th>Path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>53</td>\n      <td>54</td>\n      <td>6</td>\n      <td>5</td>\n      <td>48</td>\n      <td>49</td>\n      <td>16</td>\n      <td>Test/00000.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42</td>\n      <td>45</td>\n      <td>5</td>\n      <td>5</td>\n      <td>36</td>\n      <td>40</td>\n      <td>1</td>\n      <td>Test/00001.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48</td>\n      <td>52</td>\n      <td>6</td>\n      <td>6</td>\n      <td>43</td>\n      <td>47</td>\n      <td>38</td>\n      <td>Test/00002.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>29</td>\n      <td>5</td>\n      <td>5</td>\n      <td>22</td>\n      <td>24</td>\n      <td>33</td>\n      <td>Test/00003.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>57</td>\n      <td>5</td>\n      <td>5</td>\n      <td>55</td>\n      <td>52</td>\n      <td>11</td>\n      <td>Test/00004.png</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>52</td>\n      <td>56</td>\n      <td>5</td>\n      <td>5</td>\n      <td>47</td>\n      <td>51</td>\n      <td>38</td>\n      <td>Test/00005.png</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>147</td>\n      <td>130</td>\n      <td>12</td>\n      <td>12</td>\n      <td>135</td>\n      <td>119</td>\n      <td>18</td>\n      <td>Test/00006.png</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>32</td>\n      <td>33</td>\n      <td>5</td>\n      <td>5</td>\n      <td>26</td>\n      <td>28</td>\n      <td>12</td>\n      <td>Test/00007.png</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>45</td>\n      <td>50</td>\n      <td>6</td>\n      <td>5</td>\n      <td>40</td>\n      <td>45</td>\n      <td>25</td>\n      <td>Test/00008.png</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>81</td>\n      <td>86</td>\n      <td>7</td>\n      <td>7</td>\n      <td>74</td>\n      <td>79</td>\n      <td>35</td>\n      <td>Test/00009.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3.2. Data Loading and Preparation\n\nIn this step, we will be loading the following subsets of our dataset:\n\n- **Train** subset: This forms our training dataset, which will be used to train the model. We'll refer to the input and output data as `x_train` and `y_train` respectively.\n- **Test** subset: This is our validation dataset, used to evaluate the performance of the model during and after training. The input and output data for this subset will be referred to as `x_test` and `y_test`.\n- **Meta** subset: This subset is primarily used for visualization purposes to understand the data better. The input and output data for this subset will be referred to as `x_meta` and `y_meta`.\n\nWe'll shuffle the training data to ensure that our model isn't influenced by the order of the examples. Meanwhile, the visualization data (Meta) will be sorted to facilitate effective and easier analysis.","metadata":{}},{"cell_type":"code","source":"# ---- Read datasets\n\n(x_train,y_train) = read_csv_and_extract_road_sign_images_with_labels(f'{project_gtsrb_path}/Train.csv')\n(x_test ,y_test)  = read_csv_and_extract_road_sign_images_with_labels(f'{project_gtsrb_path}/Test.csv')\n(x_meta ,y_meta)  = read_csv_and_extract_road_sign_images_with_labels(f'{project_gtsrb_path}/Meta.csv')\n    \n# ---- Shuffle train set\n\nx_train, y_train = shuffle_np_dataset(x_train, y_train)\n\n# ---- Sort Meta\n\ncombined = list(zip(x_meta,y_meta))\ncombined.sort(key=lambda x: x[1])\nx_meta,y_meta = zip(*combined)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:03:36.986499Z","iopub.execute_input":"2023-07-17T19:03:36.986856Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, bar_style='info', max=39209)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79bc17eeec64ac89ef9d6634d14cfa2"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. Analyzing the Training Dataset\nIt is essential to understand the nature of the images in our training dataset. Homogeneity, in terms of size, ratio, width, and height, can significantly impact the effectiveness of the model.\n\n### 4.1. Conducting Statistical Analysis\nWe will perform a statistical analysis to understand the distribution and characteristics of the images in our training dataset. This analysis will provide insights into the uniformity of the images' size, their aspect ratio, and their dimensions (width and height). A balanced and homogeneous dataset tends to perform better in training machine learning models, and this analysis will help us understand how well our dataset meets these criteria.","metadata":{}},{"cell_type":"code","source":"train_size  = []\ntrain_ratio = []\ntrain_lx    = []\ntrain_ly    = []\n\ntest_size   = []\ntest_ratio  = []\ntest_lx     = []\ntest_ly     = []\n\nfor image in x_train:\n    (lx,ly,lz) = image.shape\n    train_size.append(lx*ly/1024)\n    train_ratio.append(lx/ly)\n    train_lx.append(lx)\n    train_ly.append(ly)\n\nfor image in x_test:\n    (lx,ly,lz) = image.shape\n    test_size.append(lx*ly/1024)\n    test_ratio.append(lx/ly)\n    test_lx.append(lx)\n    test_ly.append(ly)\n    \n# Displaying Dataset Shapes\nprint(f\"Shape of x_train: {x_train.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of x_test: {x_test.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2. Conducting Statistical Analysis of Distributions\n\nIn this section, we will perform a statistical analysis of the distributions present in our Kaggle dataset. The goal is to gain insights into the data and identify:\n\n- Size Statistics\n- Aspect Ratio Statistics \n- Width\n- Height\n- Class ID\n","metadata":{}},{"cell_type":"code","source":"# Size Statistics\n# ----\nplt.figure(figsize=(16,6))\nplt.hist([train_size,test_size], bins=100)\nplt.gca().set(title=f'Size Distribution (in Kpixels) - Training Data: [{min(train_size):5.2f}, {max(train_size):5.2f}]', \n              ylabel='Frequency', xlim=[0,30])\nplt.legend(['Train','Test'])\nplt.show()\n\n# Aspect Ratio Statistics (lx/ly)\n# ----\nplt.figure(figsize=(16,6))\nplt.hist([train_ratio,test_ratio], bins=100)\nplt.gca().set(title=f'Aspect Ratio (lx/ly) Distribution - Training Data: [{min(train_ratio):5.2f}, {max(train_ratio):5.2f}]', \n              ylabel='Frequency', xlim=[0.8,1.2])\nplt.legend(['Train','Test'])\nplt.show()\n\n# Width (lx) Statistics\n# ----\nplt.figure(figsize=(16,6))\nplt.hist([train_lx,test_lx], bins=100)\nplt.gca().set(title=f'Image Width (lx) Distribution - Training Data: [{min(train_lx):5.2f}, {max(train_lx):5.2f}]', \n              ylabel='Frequency', xlim=[20,150])\nplt.legend(['Train','Test'])\nplt.show()\n\n# Height (ly) Statistics\n# ----\nplt.figure(figsize=(16,6))\nplt.hist([train_ly,test_ly], bins=100)\nplt.gca().set(title=f'Image Height (ly) Distribution - Training Data: [{min(train_ly):5.2f}, {max(train_ly):5.2f}]', \n              ylabel='Frequency', xlim=[20,150])\nplt.legend(['Train','Test'])\nplt.show()\n\n# Class ID Distribution\n# ----\nplt.figure(figsize=(16,6))\nplt.hist([y_train,y_test], bins=43)\nplt.gca().set(title='Class ID Distribution', ylabel='Frequency', xlim=[0,43])\nplt.legend(['Train','Test'])\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Exploring the Image Classes\n\nIn this section, we will delve into understanding the 43 distinct classes that our images fall into. These classes represent different types of traffic signs, and each class is denoted by a unique class ID. Gaining a clear understanding of these categories will assist us in later stages of model training and prediction.\n\n### 5.1. Exploring the Meta Image Classes","metadata":{}},{"cell_type":"code","source":"show_images(\n    x_meta,\n    y_meta, \n    range(43), \n    columns=8, \n    figure_size=(2,2), \n    show_colorbar=False, \n    y_pred=None, \n    color_map='binary'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. Exploring the Data Image Classes\n\nIn this step, we aim to gain a visual understanding of our dataset. By inspecting the actual images from each class, we can have a more tangible grasp of what the traffic signs in our dataset look like, enhancing our understanding of the data we are working with.","metadata":{}},{"cell_type":"code","source":"samples = [random.randint(0,len(x_train)-1) for i in range(10)]    \nshow_data_and_sample_images(samples)\n\nsamples = [random.randint(0,len(x_train)-1) for i in range(10)]    \nshow_data_and_sample_images(samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Preprocessing the Dataset\n\nFor successful training of our model, the images in our dataset need to satisfy certain criteria. These include:\n\n- **Uniform Size**: The images must all have the same dimensions to match the input requirements of the neural network model.\n- **Normalization**: The pixel values of the images should be normalized to fall within a specific range, typically between 0 and 1. This aids in speeding up the training process and achieving better performance.\n\nWe choose to work with either **RGB** (colored)\n\n### 7.1. Enhancing Images Through Preprocessing\n\nIn this section, we will implement various preprocessing steps to enhance our images before the training process. These steps include normalization, resizing.\n\nThis will allow us to visualize the effects of image enhancement techniques on these samples.","metadata":{}},{"cell_type":"code","source":"# Randomly select 16 samples\ni = random.randint(0, len(x_train) - 16)\nx_samples = x_train[i:i+16]\ny_samples = y_train[i:i+16]\n\n# Apply image enhancement for different modes\ndatasets = preprocess_images(x_samples, width=25, height=25)\n\n# Display expected images\nshow_text(\"b\", 'EXPECTED')\nx_expected = [x_meta[i] for i in y_samples]\nshow_images(\n    x_expected, \n    y_samples, \n    range(12), \n    columns=12, \n    figure_size=(1, 1),\n    show_colorbar=False, \n    y_pred=None, \n    color_map='binary'\n)\n\n# Display original images\nshow_text(\"b\", 'ORIGINAL')\nshow_images(\n    x_samples, \n    y_samples, \n    range(12), \n    columns=12, \n    figure_size=(1, 1),\n    show_colorbar=False, \n    y_pred=None, \n    color_map='binary'\n)\n\n# Display enhanced images for each dataset\nshow_text(\"b\", 'ENHANCED')\nprint(\"Dataset: RGB  min,max=[{:.3f},{:.3f}]  shape={}\".format(datasets.min(), datasets.max(), datasets.shape))\nshow_images(\n    datasets, \n    y_samples, \n    range(12), \n    columns=12, \n    figure_size=(1, 1),\n    show_colorbar=False, \n    y_pred=None, \n    color_map='binary'\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3. Generate enhanced Datasets\nIn this section, we will generate enhanced datasets by applying feature engineering techniques to the existing data.\n\nComment: Feature engineering is an important step in any machine learning project as it helps in creating new features or transforming existing ones to improve the performance of the models.\n","metadata":{}},{"cell_type":"code","source":"n_train = int(len(x_train)*scale )\nn_test  = int(len(x_test)*scale )\n\nshow_text(\"b\",'Parameters :')\nprint(f'Scale is : {scale}')\nprint(f'x_train length is : {n_train}')\nprint(f'x_test  length is : {n_test}')\nprint(f'output dir is     : {data_output_dir}\\n')\n\nmkdir(data_output_dir)\n\ndatasets = []\nfor s in datasets_size:\n    filename = f'{data_output_dir}/set-{s}x{s}-RGB.h5'\n    datasets.append(f\"set-{s}x{s}-RGB\")\n    show_text(\"b\",f'Dataset : {filename}')\n    # Enhancement\n    #      Note : x_train is a numpy array of python objects (images with <> sizes)\n    #             but images_enhancement() return a real array of float64 numpy (images with same size)\n    #             so, we can save it in nice h5 files\n    #\n    x_train_new = preprocess_images( x_train[:n_train], width=s, height=s)\n    x_test_new  = preprocess_images( x_test[:n_test],  width=s, height=s)\n    x_meta_new  = preprocess_images( x_meta,  width=s, height=s)\n        \n    # ---- Save\n    save_h5_dataset(x_train_new, y_train[:n_train], x_test_new, y_test[:n_test], x_meta_new,y_meta, filename)\n    x_train_new, x_test_new=0,0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. Preview of Enhancing Images","metadata":{}},{"cell_type":"code","source":"samples=range(10)\n\nfor s in datasets_size:\n    filename = f'{data_output_dir}/set-{s}x{s}-RGB.h5'\n    with  h5py.File(filename,'r') as f:\n        x_tmp = f['x_train'][:]\n        y_tmp = f['y_train'][:]\n        show_text(\"b\",f'Dataset : {filename} from h5 file.')\n        show_images(\n            x_tmp,\n            y_tmp, \n            samples, \n            columns=10, \n            figure_size=(2,2), \n            show_colorbar=False, \n            y_pred=None, \n            color_map='binary'\n        )\n    x_tmp,y_tmp=0,0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Create Models","metadata":{}},{"cell_type":"markdown","source":"In this context, we have three Convolutional Neural Network (CNN) models built with Keras for image classification. Each of the models is increasingly complex, offering greater modeling capacity but also increased risk of overfitting.\n\n1. `get_model_v1(lx,ly,lz)`: This is the basic model consisting of two sets of Conv2D, MaxPooling2D, and Dropout layers, followed by a Flatten layer to convert the data into a one-dimensional form. Then, there's a Dense layer with relu activation, a Dropout layer to prevent overfitting, and finally a Dense layer with softmax activation for predicting classes.\n\n2. `get_model_v2(lx,ly,lz)`: This model is more sophisticated, with three sets of Conv2D, MaxPooling2D, and Dropout layers, with an increasing number of filters for the Conv2D layers. It also includes a Flatten layer, a Dense layer with relu activation, a Dropout layer, and a Dense layer with softmax activation.\n\n3. `get_model_v3(lx,ly,lz)`: This is the most sophisticated model, with BatchNormalization layers after Conv2D layers to speed up the convergence of learning and stabilize neuron activations. This model also includes MaxPooling2D, Dropout, Flatten, Dense layers and a final Dense layer with softmax activation.\n\nEach model takes in the image dimensions (width `lx`, height `ly`, and color depth `lz`) and returns a compiled neural network model that can be trained on image data.\n","metadata":{}},{"cell_type":"code","source":"# A basic model\n#\ndef get_model_v1(lx,ly,lz):\n    \n    model = keras.models.Sequential()\n    \n    model.add( keras.layers.Conv2D(96, (3,3), activation='relu', input_shape=(lx,ly,lz)))\n    model.add( keras.layers.MaxPooling2D((2, 2)))\n    model.add( keras.layers.Dropout(0.2))\n\n    model.add( keras.layers.Conv2D(192, (3, 3), activation='relu'))\n    model.add( keras.layers.MaxPooling2D((2, 2)))\n    model.add( keras.layers.Dropout(0.2))\n\n    model.add( keras.layers.Flatten()) \n    model.add( keras.layers.Dense(1500, activation='relu'))\n    model.add( keras.layers.Dropout(0.5))\n\n    model.add( keras.layers.Dense(43, activation='softmax'))\n    return model\n    \n# A more sophisticated model\n#\ndef get_model_v2(lx,ly,lz):\n    model = keras.models.Sequential()\n\n    model.add( keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=(lx,ly,lz), activation='relu'))\n    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add( keras.layers.Dropout(0.2))\n\n    model.add( keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add( keras.layers.Dropout(0.2))\n\n    model.add( keras.layers.Conv2D(256, (3, 3), padding='same',activation='relu'))\n    model.add( keras.layers.Conv2D(256, (3, 3), activation='relu'))\n    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add( keras.layers.Dropout(0.2))\n\n    model.add( keras.layers.Flatten())\n    model.add( keras.layers.Dense(512, activation='relu'))\n    model.add( keras.layers.Dropout(0.5))\n    model.add( keras.layers.Dense(43, activation='softmax'))\n    return model\n\n# another more sophisticated model\n#\ndef get_model_v3(lx,ly,lz):\n    model = keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same',  activation='relu', input_shape=(lx,ly,lz)))\n    model.add(tf.keras.layers.BatchNormalization(axis=-1))      \n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Dropout(0.2))\n\n    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same',  activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Dropout(0.2))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(512, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.4))\n\n    model.add(tf.keras.layers.Dense(43, activation='softmax'))\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Train Models","metadata":{}},{"cell_type":"code","source":"def multi_run(enhanced_dir, datasets, models, datagen=None,\n              scale=1, batch_size=64, epochs=16, \n              verbosity=0, tag_id='last'):\n    \"\"\"\n    Launches a dataset-model combination\n    args:\n        enhanced_dir   : Directory of the enhanced datasets\n        datasets       : List of dataset (whitout .h5)\n        models         : List of model like { \"model name\":get_model(), ...}\n        datagen        : Data generator or None (None)\n        scale          : % of dataset to use.  1 mean all. (1)\n        batch_size     : Batch size (64)\n        epochs         : Number of epochs (16)\n        verbosity  : Verbose level (0)\n        tag_id         : postfix for report, logs and models dir (_last)\n    return:\n        report        : Report as a dict for Pandas.\n    \"\"\"  \n    # Logs and models dir\n    # ---- \n    os.makedirs(f'{run_dir}/logs_{tag_id}',   mode=0o750, exist_ok=True)\n    os.makedirs(f'{run_dir}/models_{tag_id}', mode=0o750, exist_ok=True)\n    \n    # Columns of output\n    # ---- \n    output={}\n    output['Dataset'] = []\n    output['Size']    = []\n    for m in models:\n        output[m+'_Accuracy'] = []\n        output[m+'_Duration'] = []\n\n    # Each Datasets\n    # ----\n    for d_name in datasets:\n        show_text(\"h4\", f\"Dataset : {d_name}\")\n\n        # Read dataset\n        # ----\n        x_train,y_train,x_test,y_test, d_size = read_dataset(enhanced_dir, d_name)\n        output['Dataset'].append(d_name)\n        output['Size'].append(d_size)\n        \n        # Rescale\n        # ----\n        x_train,y_train,x_test,y_test = rescale_dataset(x_train,y_train,x_test,y_test, scale=scale)\n        \n        # Get the shape\n        # ----\n        (n,lx,ly,lz) = x_train.shape\n\n        # Each model\n        # ----\n        for m_name,m_function in models.items():\n            show_text(\"h5\",\"    Run model {}\".format(m_name))\n            # get model\n            # ----\n            try:\n                # get function by name\n                # ----\n                m_function=globals()[m_function]\n                model=m_function(lx,ly,lz)\n                \n                # Show it\n                # ----\n                display(visualkeras.layered_view(model, legend=True, scale_z=1, scale_xy =20, spacing=80))\n                \n                # Compile it\n                # ----\n                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n                # Callbacks tensorboard\n                # ----\n                log_dir = f'{run_dir}/logs_{tag_id}/tb_{d_name}_{m_name}'\n                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n                # Callbacks bestmodel\n                # ----\n                save_dir = f'{run_dir}/models_{tag_id}/model_{d_name}_{m_name}.h5'\n                bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, monitor='accuracy', save_best_only=True)\n                # Train\n                # ----\n                start_time = time.time()\n                if datagen==None:\n                    # ---- No data augmentation (datagen=None) --------------------------------------\n                    history = model.fit(x_train, y_train,\n                                        batch_size      = batch_size,\n                                        epochs          = epochs,\n                                        verbose         = verbosity,\n                                        validation_data = (x_test, y_test),\n                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n                else:\n                    # ---- Data augmentation (datagen given) ----------------------------------------\n                    datagen.fit(x_train)\n                    history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n                                        steps_per_epoch = int(len(x_train)/batch_size),\n                                        epochs          = epochs,\n                                        verbose         = verbosity,\n                                        validation_data = (x_test, y_test),\n                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n                    \n                # Result\n                # ----\n                end_time = time.time()\n                duration = end_time-start_time\n                accuracy = max(history.history[\"val_accuracy\"])*100\n                #\n                output[m_name+'_Accuracy'].append(accuracy)\n                output[m_name+'_Duration'].append(duration)\n                show_text(\"b\", f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n            except:\n                print('An error occured for :',m_name)\n                output[m_name+'_Accuracy'].append('0')\n                output[m_name+'_Duration'].append('999')\n                print('-')\n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Data augmentation or not\n#\nif with_datagen :\n    datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n                                                           featurewise_std_normalization=False,\n                                                           width_shift_range=0.1,\n                                                           height_shift_range=0.1,\n                                                           zoom_range=0.2,\n                                                           shear_range=0.1,\n                                                           rotation_range=10.)\nelse:\n    datagen=None\n    \n# ---- Run\n#\noutput = multi_run(data_output_dir,\n                   datasets, \n                   models,\n                   datagen       = datagen,\n                   scale         = scale,\n                   batch_size    = batch_size,\n                   epochs        = epochs,\n                   verbosity = verbosity,\n                   tag_id        = tag_id)\n\n# ---- Save report\n#\nreport={}\nreport['output']=output\nreport['description'] = f'scale={scale} batch_size={batch_size} epochs={epochs} data_aug={with_datagen}'\n\nreport_name=f'{run_dir}/report_{tag_id}.json'\n\nwith open(report_name, 'w') as file:\n    json.dump(report, file, indent=4)\n\nprint('\\nReport saved as ',report_name)\n\nprint('-'*59)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in glob.glob('./run/*.json'):\n    show_report(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}